{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Assignment2\n",
        "\n",
        "\n",
        "*   Aviral Chawla (2110110151)\n",
        "*   Aman Sagar (2110110934)\n",
        "\n"
      ],
      "metadata": {
        "id": "8tZ-VRx3mxbr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec_grXSpmvi2",
        "outputId": "e928a365-c742-4883-d8b2-d4a05f95bd5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building index...\n",
            "Index built.\n",
            "Enter your search query (or type 'exit' to quit): Developing your Zomato business account and profile is a great way to boost your restaurantâ€™s online reputation\n",
            "Top documents (Name, Score):\n",
            "zomato.txt: 0.2116\n",
            "swiggy.txt: 0.1254\n",
            "instagram.txt: 0.0577\n",
            "messenger.txt: 0.0571\n",
            "youtube.txt: 0.0472\n",
            "reddit.txt: 0.0455\n",
            "bing.txt: 0.0430\n",
            "HP.txt: 0.0416\n",
            "flipkart.txt: 0.0413\n",
            "paypal.txt: 0.0402\n",
            "Enter your search query (or type 'exit' to quit): Warwickshire, came from an ancient family and was the heiress to some land\n",
            "Top documents (Name, Score):\n",
            "shakespeare.txt: 0.1243\n",
            "levis.txt: 0.0261\n",
            "nike.txt: 0.0191\n",
            "Adobe.txt: 0.0166\n",
            "zomato.txt: 0.0154\n",
            "huawei.txt: 0.0147\n",
            "skype.txt: 0.0124\n",
            "blackberry.txt: 0.0118\n",
            "reliance.txt: 0.0110\n",
            "Dell.txt: 0.0109\n",
            "Enter your search query (or type 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Make sure to download necessary NLTK resources\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "class VSM:\n",
        "    \"\"\"\n",
        "    A class representing a Vector Space Model (VSM) for Information Retrieval (IR).\n",
        "    This implementation includes the use of lnc.ltc weighting scheme, lemmatization, and stop word removal.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, corpus_path):\n",
        "        \"\"\"\n",
        "        Initialize the VSM class.\n",
        "        :param corpus_path: Path to the directory containing text documents for the corpus.\n",
        "        \"\"\"\n",
        "        self.corpus_path = corpus_path  # Path to the corpus folder\n",
        "        self.dictionary = defaultdict(\n",
        "            list\n",
        "        )  # Inverted index: term -> list of (doc_id, tf)\n",
        "        self.doc_lengths = {}  # Stores document lengths for normalization (lnc)\n",
        "        self.doc_id_to_name = {}  # Maps document IDs to file names\n",
        "        self.N = 0  # Total number of documents in the corpus\n",
        "        self.lemmatizer = WordNetLemmatizer()  # Used for lemmatizing words\n",
        "        self.stop_words = set(\n",
        "            stopwords.words(\"english\")\n",
        "        )  # Set of stop words to be removed\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        \"\"\"\n",
        "        Preprocess a given text by tokenizing, removing stop words, lemmatizing, and eliminating punctuation.\n",
        "        :param text: The raw document text.\n",
        "        :return: A list of preprocessed terms.\n",
        "        \"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Tokenization\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Remove punctuation and non-alphabetical tokens\n",
        "        tokens = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "        # Stop word removal\n",
        "        tokens = [word for word in tokens if word not in self.stop_words]\n",
        "\n",
        "        # Lemmatization\n",
        "        tokens = [self.lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def build_index(self):\n",
        "        \"\"\"\n",
        "        Build the inverted index (dictionary) and compute document lengths for normalization.\n",
        "        \"\"\"\n",
        "        doc_id = 0  # Document ID counter\n",
        "        # Iterate over all files in the corpus directory\n",
        "        for filename in os.listdir(self.corpus_path):\n",
        "            doc_id += 1\n",
        "            self.doc_id_to_name[doc_id] = filename  # Map document ID to file name\n",
        "            # Read the document content\n",
        "            with open(\n",
        "                os.path.join(self.corpus_path, filename), \"r\", encoding=\"utf-8\"\n",
        "            ) as file:\n",
        "                try:\n",
        "                    text = file.read()  # Read the document text\n",
        "                except UnicodeDecodeError as e:\n",
        "                    print(f\"Error decoding {filename}: {e}\")\n",
        "                    continue  # Skip files that are unreadable\n",
        "\n",
        "                # Preprocess the text\n",
        "                terms = self.preprocess(text)\n",
        "                term_freq = defaultdict(\n",
        "                    int\n",
        "                )  # Dictionary to store term frequencies for the document\n",
        "\n",
        "                # Calculate term frequency\n",
        "                for term in terms:\n",
        "                    term_freq[term] += 1\n",
        "\n",
        "                # Build the postings list (term -> list of (doc_id, tf))\n",
        "                for term, tf in term_freq.items():\n",
        "                    self.dictionary[term].append((doc_id, tf))\n",
        "\n",
        "                # Calculate document length for normalization using lnc scheme\n",
        "                doc_length = math.sqrt(\n",
        "                    sum((1 + math.log10(tf)) ** 2 for tf in term_freq.values())\n",
        "                )\n",
        "                self.doc_lengths[doc_id] = doc_length  # Store the document length\n",
        "\n",
        "        self.N = doc_id  # Store the total number of documents\n",
        "\n",
        "    def calculate_document_weight(self, tf):\n",
        "        \"\"\"\n",
        "        Calculate the logarithmic term frequency (lnc) for a document.\n",
        "        :param tf: Term frequency in the document.\n",
        "        :return: Logarithmic term frequency (lnc).\n",
        "        \"\"\"\n",
        "        return 1 + math.log10(tf)  # Logarithmic scaling of term frequency\n",
        "\n",
        "    def calculate_query_weight(self, tf, df):\n",
        "        \"\"\"\n",
        "        Calculate the query term weight using logarithmic tf and inverse document frequency (ltc scheme).\n",
        "        :param tf: Term frequency in the query.\n",
        "        :param df: Document frequency (number of documents containing the term).\n",
        "        :return: The tf-idf weight for the query term.\n",
        "        \"\"\"\n",
        "        tf_weight = 1 + math.log10(tf)  # Logarithmic term frequency\n",
        "        idf = math.log10(self.N / df)  # Inverse document frequency\n",
        "        return tf_weight * idf  # Calculate tf-idf weight\n",
        "\n",
        "    def search(self, query):\n",
        "        \"\"\"\n",
        "        Search the corpus for documents most relevant to the query using cosine similarity.\n",
        "        :param query: The search query.\n",
        "        :return: A list of top 10 relevant documents and their similarity scores.\n",
        "        \"\"\"\n",
        "        query_terms = self.preprocess(query)  # Preprocess the query\n",
        "        query_vector = defaultdict(float)  # Query vector for storing weights\n",
        "        query_term_weights = {}\n",
        "\n",
        "        # Build query vector using ltc scheme\n",
        "        for term in query_terms:\n",
        "            df = len(self.dictionary.get(term, []))  # Document frequency of the term\n",
        "            if df == 0:\n",
        "                continue  # Skip terms not found in any document\n",
        "            tf = query_terms.count(term)  # Term frequency in the query\n",
        "            query_weight = self.calculate_query_weight(\n",
        "                tf, df\n",
        "            )  # Calculate tf-idf weight for the query\n",
        "            query_vector[term] = query_weight  # Store the query term weight\n",
        "            query_term_weights[term] = query_weight\n",
        "\n",
        "        # Normalize the query vector (L2 normalization)\n",
        "        query_length = math.sqrt(sum(weight**2 for weight in query_vector.values()))\n",
        "        for term in query_vector:\n",
        "            query_vector[\n",
        "                term\n",
        "            ] /= query_length  # Normalize each term's weight in the query\n",
        "\n",
        "        # Document ranking using cosine similarity\n",
        "        scores = defaultdict(float)  # Dictionary to store similarity scores\n",
        "        for term, q_weight in query_vector.items():\n",
        "            postings = self.dictionary.get(term, [])  # Get postings list for the term\n",
        "            df = len(postings)  # Document frequency\n",
        "            for doc_id, tf in postings:\n",
        "                doc_weight = self.calculate_document_weight(\n",
        "                    tf\n",
        "                )  # Calculate document weight (lnc)\n",
        "                scores[doc_id] += q_weight * (\n",
        "                    doc_weight / self.doc_lengths[doc_id]\n",
        "                )  # Cosine similarity calculation\n",
        "\n",
        "        # Sort documents by score and return the top 10\n",
        "        ranked_docs = sorted(scores.items(), key=lambda x: (-x[1], x[0]))[:10]\n",
        "\n",
        "        # Display the document names and their similarity scores\n",
        "        results = [\n",
        "            (self.doc_id_to_name[doc_id], score) for doc_id, score in ranked_docs\n",
        "        ]\n",
        "        return results\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Run the VSM model interactively for user queries.\n",
        "        \"\"\"\n",
        "        print(\"Building index...\")\n",
        "        self.build_index()  # Build the index (inverted index and document lengths)\n",
        "        print(\"Index built.\")\n",
        "        # Continuously prompt for user queries\n",
        "        while True:\n",
        "            query = input(\"Enter your search query (or type 'exit' to quit): \")\n",
        "            if query.lower() == \"exit\":  # Exit condition\n",
        "                break\n",
        "            results = self.search(query)  # Search the corpus for relevant documents\n",
        "            if not results:\n",
        "                print(\"No relevant documents found.\")\n",
        "            else:\n",
        "                print(\"Top documents (Name, Score):\")\n",
        "                for name, score in results:\n",
        "                    print(f\"{name}: {score:.4f}\")  # Display document names and scores\n",
        "\n",
        "\n",
        "# Running the model\n",
        "if __name__ == \"__main__\":\n",
        "    corpus_path = \"Corpus\"  # Path to the corpus directory (update this as needed)\n",
        "    vsm = VSM(corpus_path)  # Create the VSM object\n",
        "    vsm.run()  # Run the VSM model interactively\n"
      ]
    }
  ]
}